{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8c77aa",
   "metadata": {},
   "source": [
    "### Import dependecies and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614f21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow tensorflow-gpu opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437725ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90456b50",
   "metadata": {},
   "source": [
    "### Detection and drawing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables from Mediapipe for detection and drawing\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe6b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detection(img, model): This function process an image and make predictions about what detects (hands, face, pose)\n",
    "#     img: The image we want to process\n",
    "#     model: The model that will make the predictions\n",
    "\n",
    "def detection(img, model):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Color conversion to RGB\n",
    "    img.flags.writeable = False\n",
    "    results = model.process(img) # Image processing\n",
    "    img.flags.writeable = True\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Back to original color (BGR)\n",
    "    return img, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_landmarks(img, results): This function show the conections and landmarks of face, hands and pose. Also adds styles.\n",
    "#     img: The image we want to process\n",
    "#     results: Results given by the predictor\n",
    "\n",
    "def show_landmarks(img, results):\n",
    "    mp_drawing.draw_landmarks(\n",
    "        img, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "        mp_drawing.DrawingSpec(color=(0,0,255), thickness=1, circle_radius=1),\n",
    "        mp_drawing.DrawingSpec(color=(10,255,0), thickness=1, circle_radius=1)\n",
    "    )\n",
    "    mp_drawing.draw_landmarks(\n",
    "        img, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "         mp_drawing.DrawingSpec(color=(0,0,255), thickness=2, circle_radius=1),\n",
    "         mp_drawing.DrawingSpec(color=(234,232,24), thickness=2, circle_radius=2)\n",
    "    )\n",
    "    mp_drawing.draw_landmarks(\n",
    "        img, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "         mp_drawing.DrawingSpec(color=(0,0,255), thickness=2, circle_radius=1),\n",
    "         mp_drawing.DrawingSpec(color=(228,19,206), thickness=2, circle_radius=2)\n",
    "    )\n",
    "    mp_drawing.draw_landmarks(\n",
    "        img, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "         mp_drawing.DrawingSpec(color=(0,0,255), thickness=2, circle_radius=1),\n",
    "         mp_drawing.DrawingSpec(color=(228,19,206), thickness=2, circle_radius=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d1f795",
   "metadata": {},
   "source": [
    "### Webcam test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6d902",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "capture = cv2.VideoCapture(1)\n",
    "\n",
    "if capture.isOpened() is False: print(\"Camera is not available\")\n",
    "    \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic: # Setting mediapipe model\n",
    "    while capture.isOpened():\n",
    "        \n",
    "        # Read frames and show them\n",
    "        _, frame = capture.read()\n",
    "        \n",
    "        # Model results prediction\n",
    "        img, results = detection(frame, holistic)\n",
    "        \n",
    "        # Show landmarks\n",
    "        show_landmarks(img, results)\n",
    "        \n",
    "        cv2.imshow(\"OpenCV video\", img)\n",
    "\n",
    "        if cv2.waitKey(1) == ord(\"q\"):\n",
    "            break\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40da790",
   "metadata": {},
   "source": [
    "### Get landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052426ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_POSE_LANDMARKS = 33 * 4 # 33 landmarks. 3 coordinates and 1 visibility attribute per landmark\n",
    "NUM_FACE_LANDMARKS = 468 * 3 # 468 landmarks. 3 coordinates per landmark\n",
    "NUM_HAND_LANDMARKS = 21 * 3 # 21 landmarks. 3 coordinates per landmark\n",
    "TOTAL_LANDMARKS = NUM_POSE_LANDMARKS + NUM_FACE_LANDMARKS + NUM_HAND_LANDMARKS * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f27175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO: Optimizar'''\n",
    "\n",
    "def get_landmarks(results):\n",
    "    \n",
    "    # Face\n",
    "    face = []\n",
    "    if results.face_landmarks:\n",
    "        for result in results.face_landmarks.landmark:\n",
    "            landmark = [result.x, result.y, result.z]\n",
    "            face.append(landmark)\n",
    "        face = np.array(face).flatten()\n",
    "    else:\n",
    "        face = np.zeros(NUM_FACE_LANDMARKS)\n",
    "    face = np.array(face).flatten()\n",
    "    \n",
    "    # Pose\n",
    "    pose = []\n",
    "    if results.pose_landmarks:\n",
    "        for result in results.pose_landmarks.landmark:\n",
    "            landmark = [result.x, result.y, result.z, result.visibility]\n",
    "            pose.append(landmark)\n",
    "        pose = np.array(pose).flatten()\n",
    "    else:\n",
    "        pose = np.zeros(NUM_POSE_LANDMARKS)\n",
    "    pose = np.array(pose).flatten()\n",
    "    \n",
    "    #Left hand\n",
    "    left_hand = []\n",
    "    if results.left_hand_landmarks:\n",
    "        for result in results.left_hand_landmarks.landmark:\n",
    "            landmark = [result.x, result.y, result.z]\n",
    "            left_hand.append(landmark)\n",
    "    else:\n",
    "        left_hand = np.zeros(NUM_HAND_LANDMARKS)\n",
    "    left_hand = np.array(left_hand).flatten()\n",
    "    \n",
    "    # Right hand\n",
    "    right_hand = []\n",
    "    if results.right_hand_landmarks:\n",
    "        for result in results.right_hand_landmarks.landmark:\n",
    "            landmark = [result.x, result.y, result.z]\n",
    "            right_hand.append(landmark)\n",
    "    else:\n",
    "        right_hand = np.zeros(NUM_HAND_LANDMARKS)\n",
    "    right_hand = np.array(right_hand).flatten()\n",
    "    \n",
    "    # Return all landmarks concatenated\n",
    "    return np.concatenate([face, pose, left_hand, right_hand])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700f67a",
   "metadata": {},
   "source": [
    "### Setting up folders for datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481984fa",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4a85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # SOURCE_PATH = os.path.join(\"source\")\n",
    "# DATASET_PATH = os.path.join(\"train/dataset\")\n",
    "# signs = np.array([\"hola\", \"gracias\", \"atencion\"])\n",
    "# # signs = np.array([\"hola\", \"gracias\", \"atencion\", \"comoestas\", \"buenosdias\", \"bienvenidos\"])\n",
    "# # signs = np.array([\"comoestas\", \"buenosdias\", \"bienvenidos\"])\n",
    "\n",
    "# '''Only used on Camera Version'''\n",
    "# num_videos = 30\n",
    "# len_videos = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50858635",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1080b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join(\"test/dataset_D_8signs_60\")\n",
    "# signs = np.array([\"hola\", \"gracias\", \"atencion\"])\n",
    "signs = np.array([\"hola\", \"gracias\", \"atencion\", \"comoestas\", \"buenosdias\", \"bienvenidos\", \"porfavor\", \"adios\"])\n",
    "# signs = np.array([\"comoestas\", \"buenosdias\", \"bienvenidos\"])\n",
    "# signs = np.array([\"adios\", \"porfavor\"])\n",
    "# signs = np.array([\"adios\", \"porfavor\"])\n",
    "\n",
    "'''Only used on Camera Version'''\n",
    "num_videos = 60\n",
    "len_videos = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d9b49",
   "metadata": {},
   "source": [
    "#### Create folders for processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# '''Webcam Version'''\n",
    "# try:\n",
    "#     shutil.rmtree(DATASET_PATH)\n",
    "# except Exception as e:\n",
    "#     print(\"os.remove() failed: \", e.strerror)\n",
    "    \n",
    "for sign in signs:\n",
    "    for video_index in range(num_videos):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATASET_PATH, sign, str(video_index)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "'''Video File Version'''\n",
    "# for sign in signs:\n",
    "#     videos = os.listdir(os.path.join(SOURCE_PATH, sign))\n",
    "#     for video_index, video in enumerate(videos):\n",
    "#         try:\n",
    "#             os.makedirs(os.path.join(DATASET_PATH, sign, str(video_index)))\n",
    "#         except:\n",
    "#             pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7ce943",
   "metadata": {},
   "source": [
    "### Code for data join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1 = os.path.join(\"train/dataset_ABC_8signs\")\n",
    "PATH2 = os.path.join(\"train/dataset_ABC_8signs\")\n",
    "signs = np.array([\"hola\", \"gracias\", \"atencion\", \"comoestas\", \"buenosdias\", \"bienvenidos\",\"adios\", \"porfavor\"])\n",
    "# signs = np.array([\"hola\", \"gracias\", \"atencion\"])\n",
    "# signs = np.array([\"adios\", \"porfavor\"])\n",
    "\n",
    "for sign in signs:\n",
    "    videos = os.listdir(os.path.join(PATH2, sign))\n",
    "    videos = list(map(int, videos))\n",
    "    print(videos)\n",
    "    nextname = max(videos)\n",
    "    print(nextname)\n",
    "    videos2 = os.listdir(os.path.join(PATH1, sign))\n",
    "    videos2 = list(map(int, videos2))\n",
    "    print(videos2)\n",
    "    for video_index, video in enumerate(videos2):\n",
    "        nextname += 1\n",
    "        os.rename(os.path.join(PATH1, sign, str(video_index)), os.path.join(PATH1, sign, str(nextname)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8aa56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset folder names\n",
    "PATH = os.path.join(\"test/dataset_C_6signs\")\n",
    "# signs = np.array([\"hola\", \"gracias\", \"atencion\"])\n",
    "# signs = np.array([\"adios\", \"porfavor\"])\n",
    "signs = np.array([\"hola\", \"gracias\", \"atencion\", \"comoestas\", \"buenosdias\", \"bienvenidos\"])\n",
    "for sign in signs:\n",
    "    videos = os.listdir(os.path.join(PATH, sign))\n",
    "    videos = list(map(int, videos))\n",
    "    print(videos)\n",
    "    for video_index in range(60):\n",
    "        os.rename(os.path.join(PATH, sign, str(videos[video_index])), os.path.join(PATH, sign, str(video_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849338c",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb2fda",
   "metadata": {},
   "source": [
    "#### Camera Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7fb000",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(1)\n",
    "if not camera.isOpened(): print(\"error\")\n",
    "while camera.isOpened():\n",
    "    _, frame = camera.read()\n",
    "    cv2.imshow(\"webcam\",frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6027fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "camera = cv2.VideoCapture(1)\n",
    "\n",
    "if camera.isOpened() is False: \n",
    "    print(\"Camera is not available.\")\n",
    "else:\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic: # Setting mediapipe model\n",
    "        for sign in signs:\n",
    "            for video in range(num_videos):\n",
    "                for video_frame in range(len_videos):\n",
    "\n",
    "                    # Read frames and show\n",
    "                    _, frame = camera.read()\n",
    "\n",
    "                    # Model results prediction\n",
    "                    img, results = detection(frame, holistic)    \n",
    "\n",
    "                    # Show landmarks\n",
    "                    show_landmarks(img, results)\n",
    "\n",
    "                    # Capture funtionality\n",
    "                    if video_frame==0:\n",
    "                        cv2.rectangle(img, (0,0), (640, 40), (255, 255, 255), -1)\n",
    "                        cv2.putText(img, 'Comenzando captura de {} Video [{}]'\n",
    "                                    .format(sign.upper(), str(video)), (3,30), cv2.FONT_HERSHEY_SIMPLEX, .75, (0,0,0), 1, cv2.LINE_AA)\n",
    "                        \n",
    "                        cv2.imshow('Sign Language Recognition', img)\n",
    "                        cv2.waitKey(2000) # Wait 2sec\n",
    "                    else:\n",
    "                        cv2.imshow('Sign Language Recognition', img)\n",
    "\n",
    "                    # Landmarks saving\n",
    "                    all_landmarks = get_landmarks(results)\n",
    "                    path = os.path.join(DATASET_PATH, sign, str(video), str(video_frame))\n",
    "                    np.save(path, all_landmarks)\n",
    "\n",
    "                    if cv2.waitKey(1) == ord('q'):\n",
    "                        camera.release()\n",
    "                        cv2.destroyAllWindows()\n",
    "                        break\n",
    "                        \n",
    "                    \n",
    "        camera.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3e144",
   "metadata": {},
   "source": [
    "#### Video Files Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee89882",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "frames_length = []\n",
    "NUM_POSE_LANDMARKS = 33 * 4 # 33 landmarks. 3 coordinates and 1 visibility attribute per landmark\n",
    "NUM_FACE_LANDMARKS = 468 * 3 # 468 landmarks. 3 coordinates per landmark\n",
    "NUM_HAND_LANDMARKS = 21 * 3 # 21 landmarks. 3 coordinates per landmark\n",
    "TOTAL_LANDMARKS = NUM_POSE_LANDMARKS + NUM_FACE_LANDMARKS + NUM_HAND_LANDMARKS * 2\n",
    "\n",
    "for sign in signs:\n",
    "    videos = os.listdir(os.path.join(SOURCE_PATH, sign))\n",
    "    print(videos)\n",
    "    for video_index, video in enumerate(videos):\n",
    "        video_path = os.path.join(SOURCE_PATH, sign, video)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frames_length.append(frame_count)\n",
    "\n",
    "MAX_FRAME_LENGTH = max(frames_length)\n",
    "print(MAX_FRAME_LENGTH)\n",
    "\n",
    "\n",
    "frame_time = 1\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic: # Setting mediapipe model\n",
    "    for sign in signs:\n",
    "        videos = os.listdir(os.path.join(SOURCE_PATH, sign))\n",
    "        for video_index, video in enumerate(videos):\n",
    "            video_path = os.path.join(SOURCE_PATH, sign, video)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            print(frame_count)\n",
    "            for video_frame in range(MAX_FRAME_LENGTH):\n",
    "                # Read frames and show\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                if not ret:\n",
    "                    zero_landmarks = [0 for x in range(TOTAL_LANDMARKS)]\n",
    "                    path = os.path.join(DATASET_PATH, sign, str(video_index), str(video_frame))\n",
    "                    np.save(path, zero_landmarks)\n",
    "                    continue\n",
    "\n",
    "                # Model results prediction\n",
    "                img, results = detection(frame, holistic)    \n",
    "\n",
    "                # Show landmarks\n",
    "                show_landmarks(img, results)\n",
    "\n",
    "                # Capture funtionality\n",
    "                if video_frame == 0:\n",
    "                    cv2.putText(img, 'Comenzando captura', (150,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (60,35,239), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(img, 'Capturando frames para {} - Video [{}]'\n",
    "                                .format(sign.upper(), video), (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (60,35,239), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('Sign Language Recognition', img)\n",
    "                    cv2.waitKey(2000) # Wait 2sec\n",
    "                else:\n",
    "                    cv2.putText(img, 'Capturando frames para {} - Video [{}]'\n",
    "                                .format(sign.upper(), video), (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (60,35,239), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('Sign Language Recognition', img)\n",
    "\n",
    "                # Landmarks saving\n",
    "                all_landmarks = get_landmarks(results)\n",
    "                path = os.path.join(DATASET_PATH, sign, str(video_index), str(video_frame))\n",
    "                np.save(path, all_landmarks)\n",
    "\n",
    "                if cv2.waitKey(frame_time) == ord('q'):\n",
    "                    break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f8f6f",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd610523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos = []\n",
    "# labels = []\n",
    "# DATASET_PATH = \"dataset_fran_3signs\"\n",
    "\n",
    "# '''Webcam Version'''\n",
    "# for sign in signs:\n",
    "#     for video in range(num_videos):\n",
    "#         video_aux = []\n",
    "#         for frame in range(len_videos):\n",
    "#             frame_aux = np.load(os.path.join(DATASET_PATH, sign, str(video), \"{}.npy\".format(frame)))\n",
    "#             video_aux.append(frame_aux)\n",
    "#         videos.append(video_aux)\n",
    "#         labels.append(label_map[sign])\n",
    "\n",
    "'''Video File Version'''\n",
    "# for sign in signs:\n",
    "#     videos_list = os.listdir(os.path.join(DATASET_PATH, sign))\n",
    "#     for video in videos_list:\n",
    "#         video_aux = []\n",
    "#         frames = os.listdir(os.path.join(DATASET_PATH, sign, video))\n",
    "#         for frame in frames:\n",
    "#             frame_aux = np.load(os.path.join(DATASET_PATH, sign, str(video), frame))\n",
    "#             video_aux.append(frame_aux)\n",
    "#         videos.append(video_aux)\n",
    "#         labels.append(label_map[sign])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_POSE_LANDMARKS = 33 * 4 # 33 landmarks. 3 coordinates and 1 visibility attribute per landmark\n",
    "# NUM_FACE_LANDMARKS = 468 * 3 # 468 landmarks. 3 coordinates per landmark\n",
    "# NUM_HAND_LANDMARKS = 21 * 3 # 21 landmarks. 3 coordinates per landmark\n",
    "# TOTAL_LANDMARKS = NUM_POSE_LANDMARKS + NUM_FACE_LANDMARKS + NUM_HAND_LANDMARKS * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c900c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array(videos)\n",
    "# y = to_categorical(np.array(labels)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402586d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_videos = X.shape[0]\n",
    "# num_videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2198423",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7946c3a8",
   "metadata": {},
   "source": [
    "### 3 signs model (hola, gracias, atencion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf61e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join(\"train/dataset_AB_3signs\")\n",
    "signs = np.array([\"hola\", \"gracias\", \"atencion\"])\n",
    "\n",
    "# '''Only used on Camera Version'''\n",
    "num_videos = 60\n",
    "len_videos = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a12c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(signs)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c091cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = []\n",
    "labels = []\n",
    "\n",
    "'''Webcam Version'''\n",
    "for sign in signs:\n",
    "    for video in range(num_videos):\n",
    "        video_aux = []\n",
    "        for frame in range(len_videos):\n",
    "            frame_aux = np.load(os.path.join(DATASET_PATH, sign, str(video), \"{}.npy\".format(frame)))\n",
    "            video_aux.append(frame_aux)\n",
    "        videos.append(video_aux)\n",
    "        labels.append(label_map[sign])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(videos)\n",
    "y = to_categorical(np.array(labels)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05394b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392cf0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f86fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = os.path.join('logs/3signs_AB_3')\n",
    "tensorboard = TensorBoard(log_dir=logs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3719fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = signs.shape[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f9524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model_3signs\n",
    "model_3signs = Sequential()\n",
    "model_3signs.add(LSTM(32, return_sequences=True, activation='relu', input_shape=(len_videos,TOTAL_LANDMARKS)))\n",
    "model_3signs.add(LSTM(32, return_sequences=False, activation='relu'))\n",
    "model_3signs.add(Dense(64, activation='relu'))\n",
    "model_3signs.add(Dropout(0.2))\n",
    "model_3signs.add(Dense(32, activation='relu'))\n",
    "model_3signs.add(Dense(output, activation='softmax'))\n",
    "\n",
    "# del model_3signs\n",
    "# model_3signs = Sequential()\n",
    "# model_3signs.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(len_videos,TOTAL_LANDMARKS)))\n",
    "# model_3signs.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "# model_3signs.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "# model_3signs.add(Dense(64, activation='relu'))\n",
    "# model_3signs.add(Dense(32, activation='relu'))\n",
    "# model_3signs.add(Dense(output, activation='softmax'))\n",
    "\n",
    "model_3signs.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0779585",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading model'''\n",
    "# model_3signs.load_weights('model_3signs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b32838",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3signs.fit(X, y, epochs=4000, callbacks=[tensorboard], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3signs.save('models/model_3signs_AB.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d475eb",
   "metadata": {},
   "source": [
    "### 6 signs model (hola, gracias, atencion, como estas, buenos dÃ­as, bienvenidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join(\"train/dataset_ABC_8signs\")\n",
    "signs = np.array([\"hola\", \"gracias\", \"atencion\", \"comoestas\", \"buenosdias\", \"bienvenidos\",\"adios\", \"porfavor\"])\n",
    "\n",
    "'''Only used on Camera Version'''\n",
    "num_videos = 180\n",
    "len_videos = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(signs)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = []\n",
    "labels = []\n",
    "\n",
    "'''Webcam Version'''\n",
    "for sign in signs:\n",
    "    for video in range(num_videos):\n",
    "        video_aux = []\n",
    "        for frame in range(len_videos):\n",
    "            frame_aux = np.load(os.path.join(DATASET_PATH, sign, str(video), \"{}.npy\".format(frame)))\n",
    "            video_aux.append(frame_aux)\n",
    "        videos.append(video_aux)\n",
    "        labels.append(label_map[sign])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(videos)\n",
    "y = to_categorical(np.array(labels)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = os.path.join('logs/8signs_ABC_6')\n",
    "tensorboard = TensorBoard(log_dir=logs_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = signs.shape[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6426f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model_6signs\n",
    "\n",
    "\n",
    "model_6signs = Sequential()\n",
    "model_6signs.add(LSTM(256, return_sequences=False, activation='relu', input_shape=(len_videos,TOTAL_LANDMARKS)))\n",
    "model_6signs.add(Dense(64, activation='relu'))\n",
    "model_6signs.add(Dropout(0.1))\n",
    "model_6signs.add(Dense(32, activation='relu'))\n",
    "model_6signs.add(Dropout(0.1))\n",
    "model_6signs.add(Dense(output, activation='softmax'))\n",
    "\n",
    "# model_6signs = Sequential()\n",
    "# model_6signs.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(len_videos,TOTAL_LANDMARKS)))\n",
    "# model_6signs.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "# model_6signs.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "# model_6signs.add(Dense(64, activation='relu'))\n",
    "# model_6signs.add(Dense(output, activation='softmax'))\n",
    "\n",
    "\n",
    "model_6signs.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90581223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_6signs.fit(X, y, epochs=1000, callbacks=[tensorboard], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b97eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6signs.save('models/8S-3_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e184d22e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30827f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join(\"test/dataset_B_8signs_60\")\n",
    "# signs = np.array([\"hola\", \"gracias\", \"atencion\"])\n",
    "signs = np.array([\"hola\", \"gracias\", \"atencion\", \"comoestas\", \"buenosdias\", \"bienvenidos\", \"adios\", \"porfavor\"])\n",
    "\n",
    "# '''Only used on Camera Version'''\n",
    "num_videos = 60\n",
    "len_videos = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48975c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(signs)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c1f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = []\n",
    "labels = []\n",
    "\n",
    "'''Webcam Version'''\n",
    "for sign in signs:\n",
    "    for video in range(num_videos):\n",
    "        video_aux = []\n",
    "        for frame in range(len_videos):\n",
    "            frame_aux = np.load(os.path.join(DATASET_PATH, sign, str(video), \"{}.npy\".format(frame)))\n",
    "            video_aux.append(frame_aux)\n",
    "        videos.append(video_aux)\n",
    "        labels.append(label_map[sign])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d3581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(videos)\n",
    "y_test = to_categorical(np.array(labels)).astype(int)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"models/8S-3_v2.h5\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756d7104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels from one-hot-encoding\n",
    "y_test_lab = np.argmax(y_test, axis=1)\n",
    "y_pred_lab = np.argmax(y_pred, axis=1)\n",
    "y_test_lab = [signs[pred] for pred in y_test_lab]\n",
    "y_pred_lab = [signs[pred] for pred in y_pred_lab]\n",
    "\n",
    "# matrix = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "# multilabel_confusion_matrix(y_test_lab, y_pred_lab)\n",
    "print(y_test_lab)\n",
    "print(y_pred_lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e7846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "   y_test_lab, y_pred_lab, normalize='true', cmap='bone', xticks_rotation=60)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c466b3",
   "metadata": {},
   "source": [
    "### Real Time Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b14814",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ORIGINAL '''\n",
    "\n",
    "len_videos = 30\n",
    "video_frames = []\n",
    "sign_sequence = []\n",
    "confidence = 0.9\n",
    "model_6signs.load_weights('models/8S-3_v2.h5')\n",
    "signs = np.array([\"hola\", \"gracias\", \"atencion\", \"comoestas\", \"buenosdias\", \"bienvenidos\",\"adios\", \"porfavor\"])\n",
    "# signs = np.array([\"hola\", \"gracias\", \"atencion\"])\n",
    "# model = load_model(\"models/model_6signs_2000ep_v2.h5\")\n",
    "model = model_6signs\n",
    "out = cv2.VideoWriter('output.mp4', -1, 20.0, (640,480))\n",
    "\n",
    "camera = cv2.VideoCapture(1)\n",
    "\n",
    "if camera.isOpened() is False: print(\"Camera is not available.\")\n",
    "    \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic: # Setting mediapipe model\n",
    "    while camera.isOpened():\n",
    "        for num_frame in range(len_videos):\n",
    "            # Read frames and show them\n",
    "            _, frame = camera.read()\n",
    "\n",
    "            # Model results prediction\n",
    "            img, results = detection(frame, holistic)\n",
    "\n",
    "            # Show landmarks\n",
    "            show_landmarks(img, results)\n",
    "\n",
    "            # Predictions\n",
    "            landmarks = get_landmarks(results)\n",
    "            video_frames.append(landmarks)\n",
    "            video_frames = video_frames[-30:]\n",
    "\n",
    "            if num_frame == 0:\n",
    "                cv2.putText(img, 'Comenzando captura', (100,200), cv2.FONT_HERSHEY_SIMPLEX, .75, (0,0,255), 1, cv2.LINE_AA)\n",
    "                \n",
    "                cv2.imshow('Sign Language Recognition', img)\n",
    "                cv2.waitKey(1000) # Wait 2sec\n",
    "            else:\n",
    "                if num_frame == len_videos - 1:\n",
    "                    pred = model.predict(np.expand_dims(video_frames, axis=0))[0]\n",
    "                    sign_pred = signs[np.argmax(pred)]\n",
    "                    \n",
    "\n",
    "                    if len(sign_sequence) > 0:\n",
    "                        if sign_pred != sign_sequence[-1]:\n",
    "                            sign_sequence.append(sign_pred)\n",
    "                    else:\n",
    "                        sign_sequence.append(sign_pred)\n",
    "\n",
    "                if len(sign_sequence) > 4:\n",
    "                    sign_sequence = sign_sequence[-4:]\n",
    "                cv2.rectangle(img, (0,0), (640, 40), (255, 255, 255), -1)\n",
    "                cv2.putText(img, ' '.join(sign_sequence), (3,30), cv2.FONT_HERSHEY_SIMPLEX, .75, (0,0,0), 1, cv2.LINE_AA)\n",
    "                cv2.imshow('Sign Language Recognition', img)\n",
    "\n",
    "            out.write(img)\n",
    "\n",
    "            if cv2.waitKey(1) == ord(\"q\"):\n",
    "                camera.release()\n",
    "                break\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93191e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TEST '''\n",
    "'''\n",
    "len_videos = 30\n",
    "video_frames = []\n",
    "sign_sequence = []\n",
    "confidence = 0.9\n",
    "model_3signs.load_weights('models/3S-2.h5')\n",
    "# signs = np.array([\"hola\", \"gracias\", \"atencion\", \"comoestas\", \"buenosdias\", \"bienvenidos\",\"adios\", \"porfavor\"])\n",
    "signs = np.array([\"hola\", \"gracias\", \"atencion\"])\n",
    "# model = load_model(\"models/model_6signs_2000ep_v2.h5\")\n",
    "model = model_3signs\n",
    "out = cv2.VideoWriter('output.mp4', -1, 20.0, (640,480))\n",
    "\n",
    "camera = cv2.VideoCapture(1)\n",
    "\n",
    "if camera.isOpened() is False: print(\"Camera is not available.\")\n",
    "    \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic: # Setting mediapipe model\n",
    "    while camera.isOpened():\n",
    "        for num_frame in range(len_videos):\n",
    "            # Read frames and show them\n",
    "            _, frame = camera.read()\n",
    "\n",
    "            # Model results prediction\n",
    "            img, results = detection(frame, holistic)\n",
    "\n",
    "            # Show landmarks\n",
    "            show_landmarks(img, results)\n",
    "\n",
    "            # Predictions\n",
    "            landmarks = get_landmarks(results)\n",
    "            video_frames.append(landmarks)\n",
    "            video_frames = video_frames[-30:]\n",
    "            \n",
    "            if len(video_frames) == 30:\n",
    "                pred = model.predict(np.expand_dims(video_frames, axis=0))[0]\n",
    "                sign_pred = signs[np.argmax(pred)]\n",
    "        \n",
    "                if pred[np.argmax(pred)] > confidence:\n",
    "                    if len(sign_sequence) > 0:\n",
    "                        if sign_pred != sign_sequence[-1]:\n",
    "                            sign_sequence.append(sign_pred)\n",
    "                    else:\n",
    "                        sign_sequence.append(sign_pred)\n",
    "\n",
    "            if len(sign_sequence) > 4:\n",
    "                sign_sequence = sign_sequence[-4:]\n",
    "                \n",
    "                \n",
    "            img = show_probabilities(pred, signs, img, colors)\n",
    "            cv2.rectangle(img, (0,0), (640, 40), (255, 255, 255), -1)\n",
    "            cv2.putText(img, ' | '.join(sign_sequence), (3,30), cv2.FONT_HERSHEY_SIMPLEX, .75, (0,0,0), 1, cv2.LINE_AA)\n",
    "            cv2.imshow('Sign Language Recognition', img)\n",
    "\n",
    "\n",
    "            if cv2.waitKey(1) == ord(\"q\"):\n",
    "                camera.release()\n",
    "                break\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16),(117,245,16),(16,117,245)]\n",
    "def show_probabilities(pred, signs, img, colors):\n",
    "    output = img.copy()\n",
    "    for index, prob in enumerate(pred):\n",
    "        cv2.rectangle(output, (0,60+index*40), (int(prob*100), 90+index*40), colors[index], -1)\n",
    "        cv2.putText(output, signs[index], (0, 85+index*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(show_probabilities(results, signs, img, colors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
